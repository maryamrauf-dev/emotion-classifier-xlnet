{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed10839b-1194-4433-a6ce-985920420158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from cleantext import clean\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer, pipeline\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification\n",
    "import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "import evaluate\n",
    "\n",
    "test_data=pd.read_csv(\"./emotions_data/emotion-labels-test.csv\")\n",
    "train_data=pd.read_csv(\"./emotions_data/emotion-labels-train.csv\")\n",
    "val_data=pd.read_csv(\"./emotions_data/emotion-labels-val.csv\")\n",
    "\n",
    "data=pd.concat([test_data, train_data, val_data])\n",
    "\n",
    "\n",
    "data.head()\n",
    "\n",
    "data[\"clean-text\"]=data[\"text\"].apply(lambda x: clean(x, no_emoji=True))\n",
    "\n",
    "data[\"clean-text\"]=data[\"clean-text\"].apply(lambda x: re.sub('[^\\w\\s]',' ',x))\n",
    "\n",
    "data[\"label\"].value_counts().plot(kind=\"bar\")\n",
    "\n",
    "min_label=data.groupby(\"label\").size().min()\n",
    "data=data.groupby(\"label\").apply(lambda x: x.sample(min_label)).reset_index(drop=True)\n",
    "\n",
    "data[\"label\"].value_counts().plot(kind=\"bar\")\n",
    "\n",
    "encode=LabelEncoder()\n",
    "\n",
    "data[\"label-int\"]=encode.fit_transform(data[\"label\"])\n",
    "\n",
    "data.head()\n",
    "\n",
    "train_data,test_data=train_test_split(data, test_size=0.2, random_state=42)\n",
    "train_data, val_data=train_test_split(train_data, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "print(len(train_data)) \n",
    "print(len(test_data))\n",
    "print(len(val_data))\n",
    "\n",
    "train_df=pd.DataFrame({\"text\" : train_data[\"clean-text\"], \"label\" : train_data[\"label-int\"]})\n",
    "test_df=pd.DataFrame({\"text\" : test_data[\"clean-text\"], \"label\" : test_data[\"label-int\"]})\n",
    "val_df=pd.DataFrame({\"text\" : val_data[\"clean-text\"], \"label\" : val_data[\"label-int\"]})\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset=Dataset.from_dict(train_df)\n",
    "test_dataset=Dataset.from_dict(test_df)\n",
    "val_dataset=Dataset.from_dict(val_df)\n",
    "\n",
    "final_dict=DatasetDict({'train': train_dataset,'test' : test_dataset, 'validation': val_dataset})\n",
    "final_dict\n",
    "\n",
    "tokenizer=XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "\n",
    "\n",
    "def tokenization_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"], padding='max_length',truncation=True, max_length=128)\n",
    "    \n",
    "\n",
    "tokenizer_set=final_dict.map(tokenization_function, batched=True)\n",
    "\n",
    "sample_text=tokenizer_set[\"train\"][0][\"text\"]\n",
    "sample_text\n",
    "\n",
    "print(tokenizer_set[\"train\"][0][\"input_ids\"])\n",
    "print(tokenizer_set[\"train\"][0][\"token_type_ids\"])\n",
    "print(tokenizer_set[\"train\"][0][\"attention_mask\"])\n",
    "\n",
    "small_train_dataset=tokenizer_set[\"train\"].shuffle(seed=42).select(range(100))\n",
    "small_eval_dataset=tokenizer_set[\"test\"].shuffle(seed=42).select(range(100))\n",
    "\n",
    "print(small_train_dataset)\n",
    "print(small_eval_dataset)\n",
    "\n",
    "from transformers import XLNetForSequenceClassification\n",
    "model=XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\",\n",
    "                                                        num_labels=4,\n",
    "                                                        id2label={0:\"fear\",1:\"anger\",\n",
    "                                                                  2:\"joy\",3:\"sadness\"},\n",
    "                                                    label2id={\"fear\": 0, \"anger\": 1, \"joy\": 2, \"sadness\": 3})\n",
    "\n",
    "\n",
    "import evaluate\n",
    "metric=evaluate.load(\"accuracy\")\n",
    "\n",
    "import numpy as np\n",
    "def compute_pred(eval_pred):\n",
    "    logits,labels=eval_pred\n",
    "    predictions=np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "trainee=TrainingArguments(output_dir=\"./fresh_xlnet_base_model\",\n",
    "                          evaluation_strategy=\"epoch\",\n",
    "                          num_train_epochs=3)\n",
    "\n",
    "from transformers import Trainer\n",
    "trainer=Trainer(\n",
    "    model=model,\n",
    "    args=trainee,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_pred)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.evaluate()\n",
    "\n",
    "model.save_pretrained(\"my_finetuned_model\", safe_serialization=False)\n",
    "\n",
    "\n",
    "fine_tuned_model=XLNetForSequenceClassification.from_pretrained(\"my_finetuned_model\")\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "clf=pipeline(task=\"text-classification\",\n",
    "    model=fine_tuned_model,\n",
    "    tokenizer=tokenizer    ) \n",
    "\n",
    "clf(\"This is a random test sentence\", top_k=None)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms_course_env",
   "language": "python",
   "name": "llms_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
